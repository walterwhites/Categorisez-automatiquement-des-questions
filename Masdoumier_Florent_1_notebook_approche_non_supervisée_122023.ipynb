{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d7013fb3d50c0d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dcc1f92ef9b0e28"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import gensim\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:07:57.627981Z",
     "start_time": "2023-12-13T14:07:57.585587Z"
    }
   },
   "id": "8ae869960cf0b7a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f675b5d911b4057a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"dataset_cleaned.csv\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:07:57.634852Z",
     "start_time": "2023-12-13T14:07:57.596223Z"
    }
   },
   "id": "1b9c32568773dbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Lemmatisation Title et Body"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea332d146f67f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Charger le modèle de langue spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lemmatisation pour le champ \"title\" dans l'ensemble d'entraînement\n",
    "train_data['title_lemmatized'] = train_data['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "# Lemmatisation pour le champ \"body\" dans l'ensemble d'entraînement\n",
    "train_data['body_lemmatized'] = train_data['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "# Lemmatisation pour le champ \"title\" dans l'ensemble de test\n",
    "test_data['title_lemmatized'] = test_data['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "# Lemmatisation pour le champ \"body\" dans l'ensemble de test\n",
    "test_data['body_lemmatized'] = test_data['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9d1db703704ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3004dc1385e6e67f"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_title = CountVectorizer()\n",
    "X_title_train = vectorizer_title.fit_transform(train_data['title_lemmatized'])\n",
    "X_title_test = vectorizer_title.transform(test_data['title_lemmatized'])\n",
    "\n",
    "vectorizer_body = CountVectorizer()\n",
    "X_body_train = vectorizer_body.fit_transform(train_data['body_lemmatized'])\n",
    "X_body_test = vectorizer_body.transform(test_data['body_lemmatized'])\n",
    "\n",
    "feature_names_title = vectorizer_title.get_feature_names_out()\n",
    "word_lists_title = [feature_names_title[idx].split() for idx in X_title_train.nonzero()[1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:03.833408Z",
     "start_time": "2023-12-13T14:08:03.823187Z"
    }
   },
   "id": "c4b0eaa0a6693545"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tf-idf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f644ee32e09efc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tfidf_vectorizer_title = TfidfVectorizer()\n",
    "X_title_train_tfidf = tfidf_vectorizer_title.fit_transform(train_data['title_lemmatized'])\n",
    "\n",
    "X_title_test_tfidf = tfidf_vectorizer_title.transform(test_data['title_lemmatized'])\n",
    "\n",
    "tfidf_vectorizer_body = TfidfVectorizer()\n",
    "X_body_train_tfidf = tfidf_vectorizer_body.fit_transform(train_data['body_lemmatized'])\n",
    "\n",
    "X_body_test_tfidf = tfidf_vectorizer_body.transform(test_data['body_lemmatized'])\n",
    "\n",
    "feature_names_title = tfidf_vectorizer_title.get_feature_names_out()\n",
    "print(\"Caractéristiques (mots) apprises par le vectorizer pour 'title':\")\n",
    "print(feature_names_title)\n",
    "\n",
    "print(\"Matrice TF-IDF pour 'title' dans l'ensemble d'entraînement:\")\n",
    "print(X_title_train_tfidf.toarray())\n",
    "\n",
    "print(\"Matrice TF-IDF pour 'title' dans l'ensemble de test:\")\n",
    "print(X_title_test_tfidf.toarray())\n",
    "\n",
    "feature_names_body = tfidf_vectorizer_body.get_feature_names_out()\n",
    "print(\"Caractéristiques (mots) apprises par le vectorizer pour 'body':\")\n",
    "print(feature_names_body)\n",
    "\n",
    "print(\"Matrice TF-IDF pour 'body' dans l'ensemble d'entraînement:\")\n",
    "print(X_body_train_tfidf.toarray())\n",
    "\n",
    "print(\"Matrice TF-IDF pour 'body' dans l'ensemble de test:\")\n",
    "print(X_body_test_tfidf.toarray())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb7f25a77d26abe7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA (Latent Dirichlet allocation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b27baf680552860"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Préparer LDA sur les titres (pour un modèle simple)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8070f3db02ad7da8"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots associés à chaque topic pour le titre:\n",
      "Topic 1: 0.027*\"a\" + 0.027*\"?\" + 0.027*\"to\" + 0.027*\"*\" + 0.018*\"(\" + 0.018*\")\" + 0.018*\"in\" + 0.018*\"&\" + 0.018*\";\" + 0.018*\"how\"\n",
      "Topic 2: 0.122*\"_\" + 0.036*\"Python\" + 0.036*\"?\" + 0.028*\"do\" + 0.028*\"create\" + 0.028*\"in\" + 0.019*\"singleton\" + 0.019*\")\" + 0.019*\"(\" + 0.019*\"a\"\n",
      "Topic 3: 0.040*\"to\" + 0.040*\"in\" + 0.032*\";\" + 0.032*\"&\" + 0.025*\"with\" + 0.025*\"Pandas\" + 0.017*\"#\" + 0.017*\"how\" + 0.017*\"file\" + 0.017*\"and\"\n",
      "Topic 4: 0.046*\"?\" + 0.046*\"how\" + 0.035*\"to\" + 0.029*\"the\" + 0.024*\"I\" + 0.018*\"be\" + 0.018*\"and\" + 0.018*\"in\" + 0.018*\"a\" + 0.018*\"Python\"\n",
      "Topic 5: 0.045*\"?\" + 0.031*\"a\" + 0.023*\"why\" + 0.023*\"be\" + 0.023*\"use\" + 0.023*\")\" + 0.023*\"(\" + 0.016*\"fast\" + 0.016*\"run\" + 0.016*\"Python\"\n",
      "Perplexité pour le modèle LDA (titres) sur l'ensemble de test : -5.584511812417359\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Convertir les textes de l'ensemble de test en listes de mots\n",
    "text_data_test_title = [text.split() for text in test_data['title_lemmatized']]\n",
    "\n",
    "# dictionnaire à partir des listes de mots de l'ensemble de test\n",
    "dictionary_test_title = Dictionary(text_data_test_title)\n",
    "\n",
    "# corpus à partir de l'ensemble de test pour le titre\n",
    "corpus_test_title = [dictionary_test_title.doc2bow(word_list) for word_list in text_data_test_title]\n",
    "\n",
    "#  LDA pour le titre sur l'ensemble d'entraînement\n",
    "lda_model_title = LdaModel(corpus_test_title, num_topics=5, id2word=dictionary_test_title, passes=10, random_state=42)\n",
    "\n",
    "# Afficher les mots associés à chaque topic pour le titre\n",
    "print(\"Mots associés à chaque topic pour le titre:\")\n",
    "for topic_idx, topic in lda_model_title.print_topics():\n",
    "    print(f\"Topic {topic_idx + 1}: {topic}\")\n",
    "\n",
    "# LDA pour transformer les données de test sur l'ensemble de test\n",
    "lda_transformed_title_test = lda_model_title[corpus_test_title]\n",
    "\n",
    "# perplexité sur l'ensemble de test\n",
    "perplexity_test_title = lda_model_title.log_perplexity(corpus_test_title)\n",
    "print(f\"Perplexité pour le modèle LDA (titres) sur l'ensemble de test : {perplexity_test_title}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:03.895899Z",
     "start_time": "2023-12-13T14:08:03.839117Z"
    }
   },
   "id": "13b762b354cdcefb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trouver le nombre de topics avec la Perplexité"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f441066803039d5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre optimal de topics est : 7\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "# Convertir les textes de l'ensemble d'entraînement en listes de mots\n",
    "text_data_train_title = [text.split() for text in train_data['title_lemmatized']]\n",
    "\n",
    "# ictionnaire à partir des listes de mots de l'ensemble d'entraînement\n",
    "dictionary_train_title = Dictionary(text_data_train_title)\n",
    "\n",
    "#corpus à partir de l'ensemble d'entraînement pour le titre\n",
    "corpus_train_title = [dictionary_train_title.doc2bow(word_list) for word_list in text_data_train_title]\n",
    "\n",
    "# Liste des nombres de topics à tester\n",
    "num_topics_list = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Calculer la perplexité pour différents nombres de topics\n",
    "perplexity_scores = []\n",
    "\n",
    "for num_topics in num_topics_list:\n",
    "    lda_model_title = LdaModel(corpus_train_title, num_topics=num_topics, id2word=dictionary_train_title, passes=10, random_state=42)\n",
    "    perplexity_scores.append(lda_model_title.log_perplexity(corpus_train_title))\n",
    "\n",
    "# nombre de topics avec la perplexité la plus basse\n",
    "best_num_topics = num_topics_list[np.argmin(perplexity_scores)]\n",
    "\n",
    "print(f\"Le nombre optimal de topics est : {best_num_topics}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:05.705594Z",
     "start_time": "2023-12-13T14:08:03.898518Z"
    }
   },
   "id": "bafc6e28bd51ad3a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots associés à chaque topic pour le titre:\n",
      "Topic 1: in, quot, python, what, 39, be, use, the, and, value\n",
      "Topic 2: do, vs, in, python, and, list, how, the, be, what\n",
      "Topic 3: do, how, 39, or, use, of, pip, python, with, date\n",
      "Topic 4: to, how, be, the, and, find, file, import, between, difference\n",
      "Topic 5: in, how, 39, python, the, be, do, to, class, type\n",
      "Topic 6: dataframe, column, of, to, pandas, panda, in, how, row, change\n",
      "Topic 7: be, to, value, how, why, get, and, division, in, the\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Définir le nombre de topics\n",
    "num_topics = best_num_topics\n",
    "\n",
    "# LDA sur le titre\n",
    "lda_title = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_title.fit(X_title_train)\n",
    "\n",
    "# LDA sur le body\n",
    "lda_body = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_body.fit(X_body_train)\n",
    "\n",
    "M_topics_words = lda_title.components_\n",
    "M_quest_topics_train = lda_title.transform(X_title_train)\n",
    "\n",
    "# Afficher les mots associés à chaque topic pour le titre\n",
    "print(\"Mots associés à chaque topic pour le titre:\")\n",
    "for topic_idx, topic in enumerate(lda_title.components_):\n",
    "    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Sélectionner les 10 meilleurs mots pour chaque topic\n",
    "    top_words = [feature_names_title[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Afficher les mots associés à chaque topic pour le corps\n",
    "#print(\"\\nMots associés à chaque topic pour le corps:\")\n",
    "#for topic_idx, topic in enumerate(lda_body.components_):\n",
    " #   top_words_idx = topic.argsort()[:-10 - 1:-1]  # Sélectionner les 10 meilleurs mots pour #chaque topic\n",
    "   # top_words = [feature_names_body[i] for i in top_words_idx]\n",
    "    #print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Transformez les données de test en distributions de topics\n",
    "X_title_test_topics = lda_title.transform(X_title_test)\n",
    "X_body_test_topics = lda_body.transform(X_body_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:05.944402Z",
     "start_time": "2023-12-13T14:08:05.706338Z"
    }
   },
   "id": "fdca6ff85e8522f2"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el95351108522630569394558694\" style=\"background-color:white;\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el95351108522630569394558694_data = {\"mdsDat\": {\"x\": [-112.2518539428711, 0.5061098337173462, -24.77053451538086, 112.98651123046875, 25.26329231262207, 138.34341430664062, -137.0751190185547], \"y\": [-89.1330795288086, 4.820221424102783, 149.8689422607422, 98.71115112304688, -139.88134765625, -46.012664794921875, 55.44940185546875], \"topics\": [1, 2, 3, 4, 5, 6, 7], \"cluster\": [1, 1, 1, 1, 1, 1, 1], \"Freq\": [30.870461145873897, 23.47330916674048, 13.098808979345677, 10.822000743411692, 9.20414360940891, 7.833020882120821, 4.698255473098509]}, \"tinfo\": {\"Term\": [\"column\", \"do\", \"dataframe\", \"to\", \"39\", \"be\", \"how\", \"quot\", \"of\", \"vs\", \"in\", \"python\", \"pandas\", \"panda\", \"what\", \"value\", \"and\", \"use\", \"why\", \"find\", \"get\", \"object\", \"list\", \"row\", \"file\", \"function\", \"difference\", \"between\", \"or\", \"pip\", \"class\", \"type\", \"dictionary\", \"typeerror\", \"hint\", \"like\", \"method\", \"object\", \"bytes\", \"content\", \"handle\", \"require\", \"str\", \"style\", \"child\", \"enclose\", \"pypy\", \"assert\", \"properly\", \"pytest\", \"raise\", \"that\", \"concatenate\", \"one\", \"determine\", \"queryset\", \"deal\", \"settingwithcopywarne\", \"split\", \"select\", \"multiple\", \"into\", \"39\", \"string\", \"in\", \"python\", \"how\", \"not\", \"an\", \"the\", \"do\", \"quot\", \"be\", \"to\", \"file\", \"of\", \"with\", \"and\", \"use\", \"what\", \"panda\", \"list\", \"vs\", \"asterisk\", \"star\", \"duplicate\", \"decorator\", \"utf\", \"double\", \"comparison\", \"erlang\", \"euler\", \"haskell\", \"project\", \"speed\", \"xrange\", \"preserve\", \"while\", \"work\", \"pytz\", \"timezones\", \"comprehension\", \"lambda\", \"parameter\", \"run\", \"dump\", \"escape\", \"json\", \"save\", \"sequence\", \"font\", \"label\", \"range\", \"remove\", \"property\", \"as\", \"list\", \"function\", \"filter\", \"and\", \"do\", \"python\", \"in\", \"what\", \"the\", \"between\", \"difference\", \"with\", \"be\", \"how\", \"order\", \"call\", \"column\", \"row\", \"convert\", \"apply\", \"by\", \"dataframe\", \"base\", \"header\", \"criterion\", \"substre\", \"array\", \"numpy\", \"two\", \"change\", \"panda\", \"on\", \"other\", \"wise\", \"deleting\", \"inline\", \"ipython\", \"make\", \"notebook\", \"plot\", \"frequency\", \"tick\", \"chromedriver\", \"detect\", \"website\", \"pandas\", \"of\", \"to\", \"text\", \"value\", \"index\", \"filter\", \"how\", \"combine\", \"in\", \"function\", \"from\", \"list\", \"the\", \"find\", \"relative\", \"re\", \"conda\", \"kill\", \"recursively\", \"bat\", \"error\", \"unable\", \"vcvarsall\", \"trilli\\u043enth\", \"iloc\", \"loc\", \"101\", \"merge\", \"shuffle\", \"thread\", \"attempte\", \"even\", \"fix\", \"init__\", \"non\", \"py\", \"match\", \"search\", \"check\", \"copy\", \"average\", \"different\", \"time\", \"way\", \"import\", \"any\", \"to\", \"file\", \"be\", \"pip\", \"the\", \"how\", \"and\", \"there\", \"between\", \"difference\", \"pandas\", \"what\", \"variable\", \"you\", \"request\", \"construct\", \"must\", \"pass\", \"scalar\", \"valueerror\", \"128\", \"20\", \"ascii\", \"encode\", \"ordinal\", \"unicodeencodeerror\", \"xa0\", \"count\", \"each\", \"group\", \"groupby\", \"statistic\", \"such\", \"urllib\", \"urllib2\", \"urllib3\", \"convention\", \"cell\", \"particular\", \"named\", \"tuples\", \"newline\", \"module\", \"quot\", \"for\", \"index\", \"what\", \"use\", \"in\", \"39\", \"value\", \"python\", \"be\", \"name\", \"if\", \"the\", \"and\", \"can\", \"between\", \"difference\", \"with\", \"from\", \"django\", \"pandas\", \"dataframe\", \"to\", \"range\", \"get\", \"etc\", \"date\", \"parse\", \"super\", \"or\", \"compare\", \"either\", \"is\", \"produce\", \"result\", \"sometimes\", \"ambiguous\", \"bool\", \"empty\", \"item\", \"series\", \"truth\", \"environment\", \"inside\", \"itself\", \"my\", \"update\", \"upgrade\", \"virtual\", \"day\", \"week\", \"directory\", \"8601\", \"format\", \"iso\", \"profile\", \"pip\", \"do\", \"how\", \"39\", \"use\", \"any\", \"of\", \"why\", \"with\", \"string\", \"python\", \"the\", \"to\", \"whitespace\", \"from\", \"different\", \"division\", \"nan\", \"build\", \"current\", \"down\", \"force\", \"keep\", \"point\", \"round\", \"it\", \"mixin\", \"useful\", \"entry\", \"unique\", \"certain\", \"drop\", \"whose\", \"float\", \"replace\", \"inherit\", \"code\", \"why\", \"print\", \"return\", \"all\", \"value\", \"get\", \"can\", \"property\", \"two\", \"be\", \"to\", \"how\", \"and\", \"in\", \"list\", \"the\", \"object\", \"function\", \"of\", \"an\", \"there\", \"with\"], \"Freq\": [4.0, 7.0, 5.0, 8.0, 6.0, 8.0, 11.0, 3.0, 5.0, 3.0, 13.0, 8.0, 3.0, 2.0, 4.0, 2.0, 5.0, 2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 2.0, 2.0, 3.0, 3.0, 0.0, 1.0, 2.1394964617174566, 2.1392991359080606, 1.088420347458842, 0.8257801522036912, 0.8257313774965948, 0.8257482432593687, 1.3513930016249134, 1.5366021952038358, 0.5630319162597184, 0.5630319162597184, 0.5630319162597184, 0.5630319162597184, 0.5630319162597184, 0.5630318856551696, 0.5630318778049282, 0.5630318775758589, 0.5630318763363417, 0.5630318671929164, 0.5630318671929164, 0.5630318671929164, 0.5630318671929164, 0.5630318671929164, 0.5630318657203691, 0.5630318657203691, 0.5630318540105119, 0.563031792743066, 0.5630317906744411, 0.5630317906744411, 0.5630317882492752, 0.5623005089960417, 1.6290840137523073, 1.6140516509961909, 4.504129210185545, 1.0884582406601964, 6.542689443422006, 3.7160080657152297, 4.517088163261039, 1.0774706039150277, 1.0884948128508725, 2.8569565052908414, 2.66441132038265, 1.6138325693920743, 2.6644642166223678, 2.6628875586287113, 1.0885392340344855, 1.4873614221095517, 1.0883428851951553, 1.284318987944109, 0.9396187371854214, 1.0882451519349892, 0.8599371322584939, 0.9412128734310963, 2.9349470717788946, 1.3298978644188797, 1.3298978644188797, 1.0088879454488509, 1.0088879065989567, 0.6878781657159385, 0.687878161572855, 0.6878781584196254, 0.6878781584196254, 0.6878781584196254, 0.6878781584196254, 0.6878781584196254, 0.6878781584196254, 0.6878781570485483, 0.6878781372644087, 0.6878781372644087, 0.6878781225212297, 0.6878780776246666, 0.6878780776246666, 0.6878780465018849, 0.6878780465018849, 0.6878780269678572, 0.6877825673187864, 0.36686830210481813, 0.36686830210481813, 0.36686830210481813, 0.36686830210481813, 0.36686830210481813, 0.36686829229923223, 0.36686829229923223, 0.895402351249454, 1.0089730138876543, 0.6878238761486146, 0.687899619081982, 2.2937100223414837, 1.2301068918449476, 0.8195929615004711, 2.374792753435397, 2.9349773106055306, 2.776649930600168, 2.8440468262207514, 1.6510679039099456, 1.9720793553720135, 1.3300257190844307, 1.3300257190844307, 1.3300536505091327, 1.7777612264258646, 1.9909960913579008, 0.6881317324617222, 0.6879566914298432, 3.8917534614283875, 1.0795749520696225, 0.7281414078820219, 0.5523831464117293, 0.5523464313905084, 4.018310644018882, 0.37662485726144196, 0.3766248234976308, 0.37662481063603437, 0.37662481063603437, 0.37662481040165746, 0.37662481040165746, 0.7281545907593242, 0.8905644281007296, 1.7598970853975517, 0.5525014930323294, 0.2008665791213422, 0.2008665791213422, 0.2008665424720389, 0.20086653295834075, 0.20086653295834075, 0.20086653295834075, 0.20086653295834075, 0.20086653295834075, 0.20086653178811895, 0.20086653178811895, 0.20067984927291557, 0.20067984927291557, 0.20067984927291557, 2.0070946687003155, 2.325758011539751, 2.27764073011281, 0.37663509312542554, 0.7124190584198872, 0.37660700866329944, 0.48016308125256957, 1.5278322521109302, 0.37666061728382144, 1.5964938716369634, 0.5523994632272221, 0.5524744195317558, 0.6860717421457571, 0.6721781700688202, 1.0081373948403058, 0.6160934531451486, 0.42006371244586577, 0.4200637011769505, 0.42006369362552515, 0.4200636934217291, 0.42006368333845084, 0.42006368333845084, 0.42006368333845084, 0.42006368333845084, 0.42006368315278064, 0.4200636825715913, 0.4200636825715913, 0.4200636020956412, 0.4200636020956412, 0.4197777565549565, 0.42008646943169553, 0.2240339688430292, 0.2240339688430292, 0.2240339688430292, 0.2240339688430292, 0.2240339688430292, 0.2240339688430292, 0.22403395190320466, 0.22403395190320466, 0.22403389760244263, 0.22403386902867647, 0.22392731186204193, 0.42005300209539487, 0.42011078904526783, 0.4201082396664142, 0.6163559300612258, 0.42004990620788557, 2.2828528453408685, 0.8122817090213973, 1.4004682794306917, 0.4200441280842249, 1.3195601459741915, 1.495747005746765, 1.0083447759256263, 0.4200711003487868, 0.6162036218559734, 0.6162036218559734, 0.6158530425922749, 0.6161048266158967, 0.3654896278555733, 0.27724872474852824, 0.2772071356788135, 0.18904635356543117, 0.18904635356543117, 0.18904635356543117, 0.18904635356543117, 0.18904635356543117, 0.18904635274546164, 0.18904635274546164, 0.18904635274546164, 0.18904635274546164, 0.18904635274546164, 0.18904635274546164, 0.18904635274546164, 0.18904634189756198, 0.18904634189756198, 0.18904634189756198, 0.18904634189756198, 0.18904634189756198, 0.18904634189756198, 0.18904633909193372, 0.18904633909193372, 0.18904633909193372, 0.18904633595020684, 0.18904633225770792, 0.18904633225770792, 0.18904632212744515, 0.18904632212744515, 0.18904632043246283, 0.4537647172138279, 1.4242100476063366, 0.5419767186066485, 0.36549858537961527, 1.1595744086931314, 0.7184334291214464, 1.9476477176424236, 1.1377732905532483, 0.5664911757841917, 1.1656686534681333, 1.1244309286548066, 0.27735643447295705, 0.2772771666677282, 0.7183682833405565, 0.6079237190570349, 0.27724801344698735, 0.4536689314949404, 0.4536689314949404, 0.4536839502624268, 0.36551985577326135, 0.27722444917175393, 0.36545805844452756, 0.3653829371077882, 0.28540784237084144, 0.2202350918164926, 0.2130768615566971, 0.18905652018476982, 0.4287443654388351, 0.325254334298815, 0.3252327031191558, 0.635791882415447, 0.22176432526647136, 0.22176432526647136, 0.22176432526647136, 0.22176432526647136, 0.22176432526647136, 0.22176432526647136, 0.22176432014937464, 0.22176432014937464, 0.22176432014937464, 0.22176432014937464, 0.22176432014937464, 0.22176432014937464, 0.22176431594670418, 0.22176431594670418, 0.22176431594670418, 0.22176431594670418, 0.22176431594670418, 0.22176431594670418, 0.22176431594670418, 0.22176430870645863, 0.22176430870645863, 0.22176430849026052, 0.22176430449685622, 0.22176430449685622, 0.22176430449685622, 0.2217642779570682, 0.5322447332343243, 2.084780959361277, 1.6097693419295935, 0.9717050037323861, 0.5908470131655221, 0.3252616360225137, 0.5322626158365766, 0.3252944021688053, 0.4287807392820537, 0.32528197248766877, 0.472507012631516, 0.3250984647701698, 0.23418126413296322, 0.2217992181474792, 0.2217759071315582, 0.2217699747360698, 0.3630256940521277, 0.27535525299120717, 0.18777190597211757, 0.18777190597211757, 0.18777190342745823, 0.18777190342745823, 0.18777190342745823, 0.18777190342745823, 0.18777190342745823, 0.18777189429116714, 0.18777189429116714, 0.18777189429116714, 0.187771894194256, 0.187771894194256, 0.18772673374385174, 0.18772673374385174, 0.18772673374385174, 0.18779529492491898, 0.10008616780327165, 0.10007195934842249, 0.10003272979139177, 0.4386781837690351, 0.18779010173169125, 0.10016907082817983, 0.18778665150944926, 0.5217687627796416, 0.42693559595795105, 0.18779998037321363, 0.18778672341222857, 0.18776533883621624, 0.9768378508626656, 0.8423795018753792, 0.4894207077948561, 0.36311782460393044, 0.3326052081749102, 0.257778768776798, 0.27543643689331704, 0.21359249490942317, 0.21506470156756682, 0.22234220051536882, 0.18780952281697968, 0.18780499251029265, 0.18779325927138876], \"Total\": [4.0, 7.0, 5.0, 8.0, 6.0, 8.0, 11.0, 3.0, 5.0, 3.0, 13.0, 8.0, 3.0, 2.0, 4.0, 2.0, 5.0, 2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 2.0, 2.0, 3.0, 3.0, 0.0, 1.0, 2.278381504573901, 2.2783152494245598, 1.22736932823733, 0.9646568498153271, 0.9646272964624409, 0.9646537889405843, 1.5937139502871194, 1.87655328891486, 0.7019085977815097, 0.7019085977815097, 0.7019085977815097, 0.7019085977815097, 0.7019085977815097, 0.7019085863360596, 0.7019085832798004, 0.701908583193028, 0.7019085822906889, 0.7019085790360737, 0.7019085790360737, 0.7019085790360737, 0.7019085790360737, 0.7019085790360737, 0.7019085788363078, 0.7019085788363078, 0.7019085742391006, 0.7019085509862231, 0.7019085502822154, 0.7019085502822154, 0.7019085493608174, 0.7016664140564428, 2.109397725099085, 2.1044270489978, 6.7251152768589435, 1.5378326925049306, 13.697873945573484, 8.196483886261936, 11.643460589126766, 1.7272959049436436, 1.7860556434147632, 8.139677361709909, 7.76243162427145, 3.556397530442649, 8.19065162472967, 8.954401366399352, 2.18806176994355, 5.295370176455325, 3.7377818764201334, 5.8077318396120265, 2.7523980069562937, 4.742615369704395, 2.910018677200393, 4.410526954823534, 3.0655006732424233, 1.4604514514473366, 1.4604514514473366, 1.1394415651795335, 1.1394415440723455, 0.8184317542345133, 0.8184317519588121, 0.8184317502651626, 0.8184317502651626, 0.8184317502651626, 0.8184317502651626, 0.8184317502651626, 0.8184317502651626, 0.8184317495170556, 0.8184317388346622, 0.8184317388346622, 0.8184317308545745, 0.8184317064863281, 0.8184317064863281, 0.8184316896780068, 0.8184316896780068, 0.8184316789989866, 0.8183622646566603, 0.4974218978929919, 0.4974218978929919, 0.4974218978929919, 0.4974218978929919, 0.4974218978929919, 0.49742189258528136, 0.49742189258528136, 1.2335879701105674, 1.4022052151914806, 0.9936460898265739, 0.9948906116192482, 4.410526954823534, 2.266880394948548, 1.4053567533387792, 5.8077318396120265, 7.76243162427145, 8.196483886261936, 13.697873945573484, 4.742615369704395, 8.139677361709909, 3.453181507103497, 3.453181507103497, 3.7377818764201334, 8.19065162472967, 11.643460589126766, 1.7360016277981076, 1.4474659594340962, 4.043088487829844, 1.2309339478878922, 0.8794452503182927, 0.7036869633765298, 0.7037051114294486, 5.121792160875104, 0.527928675775458, 0.5279286748656026, 0.5279286747601588, 0.5279286747601588, 0.5279286746525068, 0.5279286746525068, 1.0547056672787885, 1.428073706298231, 2.910018677200393, 0.9663765566413002, 0.35217038846934073, 0.35217038846934073, 0.3521703875272221, 0.35217038732124206, 0.35217038732124206, 0.35217038732124206, 0.35217038732124206, 0.35217038732124206, 0.3521703872946846, 0.3521703872946846, 0.3520774523439536, 0.3520774523439536, 0.3520774523439536, 3.8150119502756756, 5.295370176455325, 8.954401366399352, 0.8489300002951019, 2.1338153281523367, 0.8808063788780883, 1.4053567533387792, 11.643460589126766, 1.0534074242598255, 13.697873945573484, 2.266880394948548, 2.5186457845282737, 4.410526954823534, 8.139677361709909, 1.1565592439967587, 0.7645013501052045, 0.56847159970798, 0.5684715980000635, 0.5684715967761682, 0.5684715967318495, 0.5684715952831816, 0.5684715952831816, 0.5684715952831816, 0.5684715952831816, 0.5684715952858237, 0.5684715951491691, 0.5684715951491691, 0.5684715831134967, 0.5684715831134967, 0.5684420036763951, 0.6561111114828047, 0.3724418488200502, 0.3724418488200502, 0.3724418488200502, 0.3724418488200502, 0.3724418488200502, 0.3724418488200502, 0.3724418463091077, 0.3724418463091077, 0.37244183800820235, 0.3724418337543675, 0.3724308122757053, 0.775446620046843, 0.8312037799477616, 0.8312046572470756, 1.3783477027702196, 0.8789351868034171, 8.954401366399352, 2.18806176994355, 8.19065162472967, 1.0859125210619924, 8.139677361709909, 11.643460589126766, 5.8077318396120265, 1.3856522246421958, 3.453181507103497, 3.453181507103497, 3.8150119502756756, 4.742615369704395, 0.5292986896271273, 0.4410961629148277, 0.4410766776458573, 0.3528554020683084, 0.3528554020683084, 0.3528554020683084, 0.3528554020683084, 0.3528554020683084, 0.3528554030218217, 0.3528554030218217, 0.3528554030218217, 0.3528554030218217, 0.3528554030218217, 0.3528554030218217, 0.3528554030218217, 0.35285541442923934, 0.35285541442923934, 0.35285541442923934, 0.35285541442923934, 0.35285541442923934, 0.35285541442923934, 0.3528554175325165, 0.3528554175325165, 0.3528554175325165, 0.35285542088267724, 0.3528554246437177, 0.3528554246437177, 0.35285543547803194, 0.35285543547803194, 0.3528554372960432, 0.8801627781090634, 3.556397530442649, 1.4187068493217214, 0.8808063788780883, 4.742615369704395, 2.7523980069562937, 13.697873945573484, 6.7251152768589435, 2.1338153281523367, 8.196483886261936, 8.19065162472967, 0.703542246997442, 0.70379158876736, 8.139677361709909, 5.8077318396120265, 0.879043531954266, 3.453181507103497, 3.453181507103497, 3.7377818764201334, 2.5186457845282737, 1.1625726908678324, 3.8150119502756756, 5.121792160875104, 8.954401366399352, 1.2335879701105674, 1.8749396399841072, 0.6155834999528569, 0.5903722397135591, 0.4868822145329196, 0.4868790230109298, 0.9730634764597544, 0.3833921725014376, 0.3833921725014376, 0.3833921725014376, 0.3833921725014376, 0.3833921725014376, 0.3833921725014376, 0.383392176255067, 0.383392176255067, 0.383392176255067, 0.383392176255067, 0.383392176255067, 0.383392176255067, 0.3833921797129293, 0.3833921797129293, 0.3833921797129293, 0.3833921797129293, 0.3833921797129293, 0.3833921797129293, 0.3833921797129293, 0.38339218519531654, 0.38339218519531654, 0.3833921855742088, 0.38339218850500023, 0.38339218850500023, 0.38339218850500023, 0.3833922092378952, 1.0859125210619924, 7.76243162427145, 11.643460589126766, 6.7251152768589435, 2.7523980069562937, 0.8789351868034171, 5.295370176455325, 1.3884428901633226, 3.7377818764201334, 1.5378326925049306, 8.196483886261936, 8.139677361709909, 8.954401366399352, 0.6460866095168323, 2.5186457845282737, 0.775446620046843, 0.526919695679716, 0.4393366015635686, 0.35166590469262965, 0.35166590469262965, 0.35166590771479295, 0.35166590771479295, 0.35166590771479295, 0.35166590771479295, 0.35166590771479295, 0.35166591870591346, 0.35166591870591346, 0.35166591870591346, 0.35166591889803367, 0.35166591889803367, 0.35171133967360113, 0.35171133967360113, 0.35171133967360113, 0.45515174079462484, 0.26409820110886173, 0.2641849196427567, 0.264337955341678, 1.3884428901633226, 0.6726272455082882, 0.36772544707496746, 0.7350879803706662, 2.1338153281523367, 1.8749396399841072, 0.879043531954266, 0.9936460898265739, 1.0547056672787885, 8.19065162472967, 8.954401366399352, 11.643460589126766, 5.8077318396120265, 13.697873945573484, 4.410526954823534, 8.139677361709909, 1.87655328891486, 2.266880394948548, 5.295370176455325, 1.7860556434147632, 1.3856522246421958, 3.7377818764201334], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.8901, -3.8902, -4.5659, -4.8421, -4.8421, -4.8421, -4.3495, -4.2211, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.225, -5.2263, -4.1626, -4.1719, -3.1456, -4.5659, -2.7723, -3.338, -3.1428, -4.576, -4.5658, -3.6009, -3.6706, -4.172, -3.6706, -3.6712, -4.5658, -4.2536, -4.566, -4.4004, -4.7129, -4.5661, -4.8015, -4.7112, -3.3, -4.0916, -4.0916, -4.3678, -4.3678, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.7508, -4.751, -5.3794, -5.3794, -5.3794, -5.3794, -5.3794, -5.3794, -5.3794, -4.4872, -4.3678, -4.7509, -4.7508, -3.5465, -4.1696, -4.5756, -3.5118, -3.3, -3.3554, -3.3315, -3.8753, -3.6976, -4.0915, -4.0915, -4.0915, -3.8013, -3.6881, -4.7505, -4.7507, -2.4345, -3.7168, -4.1106, -4.3869, -4.3869, -2.4025, -4.7699, -4.7699, -4.7699, -4.7699, -4.7699, -4.7699, -4.1106, -3.9093, -3.2281, -4.3867, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3985, -5.3994, -5.3994, -5.3994, -3.0967, -2.9493, -2.9702, -4.7698, -4.1324, -4.7699, -4.527, -3.3695, -4.7698, -3.3255, -4.3868, -4.3867, -4.1701, -4.1906, -3.5943, -4.0868, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4698, -4.4704, -4.4697, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0984, -5.0988, -4.4698, -4.4696, -4.4697, -4.0863, -4.4698, -2.777, -3.8103, -3.2656, -4.4698, -3.3251, -3.1998, -3.5941, -4.4697, -4.0866, -4.0866, -4.0872, -4.0867, -4.447, -4.7233, -4.7235, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -5.1062, -4.2307, -3.0869, -4.053, -4.447, -3.2924, -3.7712, -2.7739, -3.3114, -4.0088, -3.2872, -3.3232, -4.7229, -4.7232, -3.7713, -3.9382, -4.7233, -4.2309, -4.2309, -4.2308, -4.4469, -4.7234, -4.4471, -4.4473, -4.6943, -4.9535, -4.9866, -5.1062, -4.1261, -4.4023, -4.4024, -3.7321, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -4.7853, -3.9098, -2.5445, -2.8031, -3.3079, -3.8054, -4.4023, -3.9098, -4.4022, -4.126, -4.4022, -4.0289, -4.4028, -4.7308, -4.7852, -4.7853, -4.7853, -3.7813, -4.0577, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4405, -4.4408, -4.4408, -4.4408, -4.4404, -5.0697, -5.0699, -5.0703, -3.592, -4.4405, -5.0689, -4.4405, -3.4186, -3.6191, -4.4404, -4.4405, -4.4406, -2.7915, -2.9395, -3.4826, -3.781, -3.8688, -4.1237, -4.0574, -4.3117, -4.3048, -4.2716, -4.4403, -4.4404, -4.4404], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1125, 1.1124, 1.0552, 1.0199, 1.0199, 1.0199, 1.0104, 0.9755, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9549, 0.9539, 0.917, 0.9101, 0.7745, 0.8298, 0.4365, 0.3843, 0.2285, 0.7034, 0.6802, 0.1284, 0.1061, 0.3852, 0.0524, -0.0374, 0.4772, -0.0945, -0.0585, -0.3336, 0.1006, -0.2967, -0.0437, -0.3692, 1.4058, 1.3557, 1.3557, 1.3276, 1.3276, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.2755, 1.1449, 1.1449, 1.1449, 1.1449, 1.1449, 1.1449, 1.1449, 1.1289, 1.1202, 1.0815, 1.0803, 0.7955, 0.838, 0.9101, 0.555, 0.4767, 0.3668, -0.1227, 0.3941, 0.0316, 0.4952, 0.4952, 0.416, -0.0783, -0.3168, 0.5239, 0.7055, 1.9945, 1.9014, 1.8439, 1.7906, 1.7905, 1.79, 1.6949, 1.6949, 1.6949, 1.6949, 1.6949, 1.6949, 1.6621, 1.5604, 1.5297, 1.4736, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4712, 1.4705, 1.4705, 1.4705, 1.3904, 1.2099, 0.6636, 1.2199, 0.9356, 1.183, 0.9587, 0.0018, 1.0042, -0.1168, 0.6208, 0.5156, 0.1719, -0.4613, 2.0862, 2.0078, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.921, 1.9204, 1.7777, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7153, 1.7149, 1.6105, 1.5412, 1.5412, 1.4188, 1.4853, 0.8569, 1.2327, 0.4574, 1.2738, 0.4041, 0.1715, 0.4727, 1.0301, 0.5001, 0.5001, 0.3999, 0.1827, 2.0152, 1.9212, 1.9211, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.7615, 1.723, 1.4704, 1.4232, 1.5059, 0.977, 1.0424, 0.4349, 0.6087, 1.0593, 0.4351, 0.3998, 1.4547, 1.4541, -0.042, 0.1286, 1.2316, 0.3558, 0.3558, 0.2767, 0.4554, 0.952, 0.04, -0.2548, -1.0605, 0.6625, 0.2108, 1.205, 2.2269, 2.1434, 2.1433, 2.1212, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.9994, 1.8337, 1.2322, 0.5682, 0.6123, 1.0082, 1.5527, 0.2494, 1.0956, 0.3815, 0.9934, -0.3066, -0.6736, -1.097, 1.4777, 0.117, 1.295, 2.6854, 2.5908, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4305, 2.4302, 2.4302, 2.4302, 2.1727, 2.0877, 2.0872, 2.0862, 1.9058, 1.7821, 1.7575, 1.6933, 1.6495, 1.5783, 1.5145, 1.3919, 1.3322, 0.9316, 0.6943, -0.1113, 0.2858, -0.6601, 0.2183, -0.3282, 0.8849, 0.7028, -0.1124, 0.8056, 1.0595, 0.0671]}, \"token.table\": {\"Topic\": [1, 5, 6, 1, 1, 2, 4, 5, 3, 2, 1, 2, 1, 2, 4, 5, 7, 1, 2, 4, 3, 1, 1, 2, 3, 1, 1, 3, 1, 2, 2, 1, 1, 3, 1, 3, 1, 2, 1, 1, 1, 2, 4, 1, 1, 2, 6, 2, 2, 1, 2, 2, 1, 4, 2, 4, 5, 1, 2, 3, 2, 3, 1, 1, 2, 1, 1, 2, 3, 4, 6, 1, 4, 1, 2, 3, 5, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 3, 6, 3, 1, 6, 1, 2, 1, 3, 1, 3, 4, 2, 6, 2, 2, 1, 2, 1, 1, 1, 2, 5, 2, 1, 1, 5, 1, 2, 4, 2, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 3, 4, 5, 2, 2, 1, 3, 4, 7, 3, 1, 1, 1, 5, 6, 2, 3, 5, 7, 2, 1, 2, 4, 5, 2, 1, 2, 2, 2], \"Freq\": [0.7434816793706053, 0.14869633587412104, 0.14869633587412104, 0.5598929706848875, 0.17218425843621646, 0.3443685168724329, 0.17218425843621646, 0.17218425843621646, 1.421086437642186, 1.0051356283003172, 1.4246869604775214, 0.6847197823720741, 0.36627122449479277, 0.24418081632986186, 0.12209040816493093, 0.12209040816493093, 0.12209040816493093, 0.2895880213486932, 0.2895880213486932, 0.2895880213486932, 1.4210497888365232, 1.4246869224292937, 0.6908625335762382, 0.6908625335762382, 0.7002439689140005, 1.4246869518638896, 0.8778161146344263, 0.9893426799933899, 0.94930031531024, 1.2218489809028197, 1.221849071354296, 1.424686960882993, 1.4246869224292937, 1.1370804488830608, 0.19524415840980575, 0.780976633639223, 1.424687018840177, 0.8776229067671311, 1.4246869702140958, 0.8147506842428078, 0.2895880213486932, 0.2895880213486932, 0.2895880213486932, 0.8601612680696328, 0.3864768342203037, 0.3864768342203037, 0.2576512228135358, 1.2218489783743451, 0.8776228905099115, 1.4246869520400145, 1.2218489809028197, 1.2218489809028197, 0.45702548883060057, 0.45702548883060057, 0.7115630942991863, 0.8646336149147605, 0.7048672532159103, 0.397038760330204, 0.397038760330204, 0.397038760330204, 0.44113487514752503, 0.44113487514752503, 0.5333505029572453, 1.4246869224292937, 1.2218489809028197, 1.0366698139968469, 0.4294255957433518, 0.17177023829734073, 0.17177023829734073, 0.08588511914867036, 0.17177023829734073, 0.7255063421154098, 0.7255063421154098, 0.511028209765507, 0.21901208989950302, 0.146008059933002, 0.146008059933002, 0.9503774440422957, 1.221849071354296, 1.0366413437283382, 0.2267302774119448, 0.4534605548238896, 0.2267302774119448, 0.6274651732952721, 0.948137933497607, 0.5789395998322748, 1.065783749288849, 0.18884421044751046, 0.18884421044751046, 0.3776884208950209, 0.18884421044751046, 1.034793314394505, 1.424686960882993, 1.0276821853783344, 0.5760363262264736, 0.5760363262264736, 0.34364040610284263, 0.6872808122056853, 0.2621223768192231, 0.5242447536384462, 0.2621223768192231, 1.2218490872971672, 0.9208844917102785, 1.2218489979675846, 1.2218489809028197, 1.4246869604775214, 1.0063945405094232, 1.4246869538715219, 1.4246869604775214, 0.488014135756964, 0.366010601817723, 0.122003533939241, 1.2218490462608989, 1.4246870174112294, 0.5623668284774309, 0.28118341423871546, 1.4246869604775214, 0.8106434435400414, 1.3080421635127106, 0.7131623739278729, 1.4246869224292937, 0.8123912755155205, 1.2219527258133596, 1.4251786603534924, 1.424687018840177, 1.2218489809028197, 1.4246870207103692, 0.6847197823720741, 1.4246869224292937, 0.6502657960607726, 1.4246869456604998, 1.4246869604775214, 0.3685649770484008, 0.24570998469893385, 0.12285499234946692, 0.12285499234946692, 0.12285499234946692, 0.7216818060233121, 1.2218490462608989, 0.33503077171157986, 0.22335384780771989, 0.22335384780771989, 0.11167692390385994, 0.9481318163199666, 0.877841642198175, 1.0366380544454112, 0.3633195480714063, 0.3633195480714063, 0.3633195480714063, 1.2218489749769168, 0.46864411685799284, 0.46864411685799284, 0.46864411685799284, 0.9786329607381419, 0.2108541220500303, 0.4217082441000606, 0.2108541220500303, 0.2108541220500303, 1.2218489979675846, 0.267538351102968, 0.267538351102968, 1.2218490098811772, 1.2218489820196798], \"Term\": [\"39\", \"39\", \"39\", \"an\", \"and\", \"and\", \"and\", \"and\", \"apply\", \"as\", \"assert\", \"asterisk\", \"be\", \"be\", \"be\", \"be\", \"be\", \"between\", \"between\", \"between\", \"by\", \"bytes\", \"call\", \"call\", \"change\", \"child\", \"class\", \"column\", \"combine\", \"comparison\", \"comprehension\", \"concatenate\", \"content\", \"convert\", \"dataframe\", \"dataframe\", \"deal\", \"decorator\", \"determine\", \"dictionary\", \"difference\", \"difference\", \"difference\", \"django\", \"do\", \"do\", \"do\", \"double\", \"duplicate\", \"enclose\", \"erlang\", \"euler\", \"file\", \"file\", \"filter\", \"find\", \"for\", \"from\", \"from\", \"from\", \"function\", \"function\", \"get\", \"handle\", \"haskell\", \"hint\", \"how\", \"how\", \"how\", \"how\", \"how\", \"import\", \"import\", \"in\", \"in\", \"in\", \"in\", \"into\", \"lambda\", \"like\", \"list\", \"list\", \"list\", \"method\", \"multiple\", \"not\", \"object\", \"of\", \"of\", \"of\", \"of\", \"on\", \"one\", \"or\", \"order\", \"order\", \"panda\", \"panda\", \"pandas\", \"pandas\", \"pandas\", \"parameter\", \"pip\", \"preserve\", \"project\", \"properly\", \"property\", \"pypy\", \"pytest\", \"python\", \"python\", \"python\", \"pytz\", \"queryset\", \"quot\", \"quot\", \"raise\", \"range\", \"relative\", \"remove\", \"require\", \"row\", \"run\", \"select\", \"settingwithcopywarne\", \"speed\", \"split\", \"star\", \"str\", \"string\", \"style\", \"that\", \"the\", \"the\", \"the\", \"the\", \"the\", \"there\", \"timezones\", \"to\", \"to\", \"to\", \"to\", \"two\", \"type\", \"typeerror\", \"use\", \"use\", \"use\", \"utf\", \"value\", \"value\", \"value\", \"vs\", \"what\", \"what\", \"what\", \"what\", \"while\", \"with\", \"with\", \"work\", \"xrange\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 2, 6, 4, 1, 3, 7]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el95351108522630569394558694\", ldavis_el95351108522630569394558694_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el95351108522630569394558694\", ldavis_el95351108522630569394558694_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el95351108522630569394558694\", ldavis_el95351108522630569394558694_data);\n            })\n         });\n}\n</script>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "# Visualisation pour le titre\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_title = pyLDAvis.lda_model.prepare(lda_title, X_title_test, vectorizer_title, mds='tsne')\n",
    "pyLDAvis.display(vis_title)\n",
    "\n",
    "# Visualisation pour le body\n",
    "# vis_body = pyLDAvis.sklearn.prepare(lda_body, X_body_test, vectorizer_body, mds='tsne')\n",
    "# pyLDAvis.display(vis_body)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:07.584265Z",
     "start_time": "2023-12-13T14:08:05.945109Z"
    }
   },
   "id": "f8592bca02c2f0b"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: in, quot, python, what, 39, be, use, the, and, value\n",
      "Topic 2: do, vs, in, python, and, list, how, the, be, what\n",
      "Topic 3: do, how, 39, or, use, of, pip, python, with, date\n",
      "Topic 4: to, how, be, the, and, find, file, import, between, difference\n",
      "Topic 5: in, how, 39, python, the, be, do, to, class, type\n",
      "Topic 6: dataframe, column, of, to, pandas, panda, in, how, row, change\n",
      "Topic 7: be, to, value, how, why, get, and, division, in, the\n"
     ]
    }
   ],
   "source": [
    "num_topics = M_topics_words.shape[0]\n",
    "num_top_words = 10\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    top_words_idx = M_topics_words[topic_idx].argsort()[:-num_top_words - 1:-1]\n",
    "    top_words = [feature_names_title[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:07.589790Z",
     "start_time": "2023-12-13T14:08:07.586338Z"
    }
   },
   "id": "c6ad72a1ddd9e31"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 - Topics Principaux : 4, 0, 3\n",
      "Question 2 - Topics Principaux : 4, 5, 3\n",
      "Question 3 - Topics Principaux : 1, 3, 0\n",
      "Question 4 - Topics Principaux : 3, 6, 5\n",
      "Question 5 - Topics Principaux : 2, 6, 0\n",
      "Question 6 - Topics Principaux : 0, 5, 6\n",
      "Question 7 - Topics Principaux : 5, 6, 3\n",
      "Question 8 - Topics Principaux : 4, 2, 1\n",
      "Question 9 - Topics Principaux : 4, 6, 2\n",
      "Question 10 - Topics Principaux : 1, 0, 4\n",
      "Question 11 - Topics Principaux : 1, 4, 2\n",
      "Question 12 - Topics Principaux : 6, 4, 5\n",
      "Question 13 - Topics Principaux : 4, 0, 3\n",
      "Question 14 - Topics Principaux : 3, 2, 4\n",
      "Question 15 - Topics Principaux : 2, 6, 4\n",
      "Question 16 - Topics Principaux : 4, 0, 3\n",
      "Question 17 - Topics Principaux : 2, 6, 5\n",
      "Question 18 - Topics Principaux : 3, 4, 5\n",
      "Question 19 - Topics Principaux : 3, 5, 0\n",
      "Question 20 - Topics Principaux : 0, 3, 6\n",
      "Question 21 - Topics Principaux : 2, 0, 1\n",
      "Question 22 - Topics Principaux : 3, 0, 1\n",
      "Question 23 - Topics Principaux : 4, 5, 3\n",
      "Question 24 - Topics Principaux : 6, 0, 4\n",
      "Question 25 - Topics Principaux : 0, 4, 3\n",
      "Question 26 - Topics Principaux : 6, 5, 4\n",
      "Question 27 - Topics Principaux : 4, 5, 0\n",
      "Question 28 - Topics Principaux : 6, 0, 1\n",
      "Question 29 - Topics Principaux : 0, 4, 2\n",
      "Question 30 - Topics Principaux : 4, 0, 1\n",
      "Question 31 - Topics Principaux : 6, 5, 4\n",
      "Question 32 - Topics Principaux : 1, 0, 4\n",
      "Question 33 - Topics Principaux : 3, 6, 2\n",
      "Question 34 - Topics Principaux : 3, 2, 4\n",
      "Question 35 - Topics Principaux : 5, 6, 1\n",
      "Question 36 - Topics Principaux : 1, 0, 3\n",
      "Question 37 - Topics Principaux : 0, 1, 4\n",
      "Question 38 - Topics Principaux : 4, 0, 5\n",
      "Question 39 - Topics Principaux : 5, 3, 6\n",
      "Question 40 - Topics Principaux : 3, 0, 4\n",
      "Question 41 - Topics Principaux : 0, 4, 1\n",
      "Question 42 - Topics Principaux : 5, 3, 4\n",
      "Question 43 - Topics Principaux : 0, 3, 6\n",
      "Question 44 - Topics Principaux : 6, 5, 0\n",
      "Question 45 - Topics Principaux : 4, 2, 1\n",
      "Question 46 - Topics Principaux : 2, 1, 4\n",
      "Question 47 - Topics Principaux : 5, 3, 0\n",
      "Question 48 - Topics Principaux : 0, 1, 6\n",
      "Question 49 - Topics Principaux : 1, 5, 6\n",
      "Question 50 - Topics Principaux : 0, 4, 2\n",
      "Question 51 - Topics Principaux : 6, 3, 5\n",
      "Question 52 - Topics Principaux : 4, 2, 1\n",
      "Question 53 - Topics Principaux : 5, 4, 0\n",
      "Question 54 - Topics Principaux : 2, 4, 0\n",
      "Question 55 - Topics Principaux : 6, 0, 1\n",
      "Question 56 - Topics Principaux : 2, 4, 6\n",
      "Question 57 - Topics Principaux : 3, 0, 4\n",
      "Question 58 - Topics Principaux : 1, 5, 6\n",
      "Question 59 - Topics Principaux : 5, 4, 0\n",
      "Question 60 - Topics Principaux : 1, 4, 2\n",
      "Question 61 - Topics Principaux : 1, 0, 6\n",
      "Question 62 - Topics Principaux : 5, 6, 4\n",
      "Question 63 - Topics Principaux : 4, 6, 0\n",
      "Question 64 - Topics Principaux : 1, 0, 3\n",
      "Question 65 - Topics Principaux : 5, 4, 0\n",
      "Question 66 - Topics Principaux : 0, 5, 6\n",
      "Question 67 - Topics Principaux : 4, 2, 5\n",
      "Question 68 - Topics Principaux : 1, 6, 4\n",
      "Question 69 - Topics Principaux : 1, 0, 4\n",
      "Question 70 - Topics Principaux : 4, 5, 3\n",
      "Question 71 - Topics Principaux : 6, 4, 5\n",
      "Question 72 - Topics Principaux : 3, 6, 5\n",
      "Question 73 - Topics Principaux : 2, 4, 0\n",
      "Question 74 - Topics Principaux : 3, 5, 1\n",
      "Question 75 - Topics Principaux : 1, 4, 0\n",
      "Question 76 - Topics Principaux : 4, 1, 0\n",
      "Question 77 - Topics Principaux : 3, 5, 0\n",
      "Question 78 - Topics Principaux : 2, 5, 1\n",
      "Question 79 - Topics Principaux : 1, 0, 4\n",
      "Question 80 - Topics Principaux : 0, 5, 6\n",
      "Question 81 - Topics Principaux : 0, 2, 1\n",
      "Question 82 - Topics Principaux : 6, 3, 5\n",
      "Question 83 - Topics Principaux : 4, 1, 3\n",
      "Question 84 - Topics Principaux : 5, 1, 0\n",
      "Question 85 - Topics Principaux : 4, 5, 3\n",
      "Question 86 - Topics Principaux : 2, 5, 1\n",
      "Question 87 - Topics Principaux : 3, 0, 1\n",
      "Question 88 - Topics Principaux : 3, 6, 2\n",
      "Question 89 - Topics Principaux : 0, 4, 1\n",
      "Question 90 - Topics Principaux : 2, 4, 1\n",
      "Question 91 - Topics Principaux : 2, 4, 1\n",
      "Question 92 - Topics Principaux : 1, 0, 6\n",
      "Question 93 - Topics Principaux : 5, 1, 0\n",
      "Question 94 - Topics Principaux : 4, 5, 2\n",
      "Question 95 - Topics Principaux : 2, 4, 1\n",
      "Question 96 - Topics Principaux : 4, 6, 2\n",
      "Question 97 - Topics Principaux : 4, 2, 5\n",
      "Question 98 - Topics Principaux : 0, 4, 3\n",
      "Question 99 - Topics Principaux : 5, 3, 0\n",
      "Question 100 - Topics Principaux : 1, 5, 6\n",
      "Question 101 - Topics Principaux : 2, 4, 6\n",
      "Question 102 - Topics Principaux : 2, 1, 3\n",
      "Question 103 - Topics Principaux : 5, 4, 0\n",
      "Question 104 - Topics Principaux : 3, 6, 2\n",
      "Question 105 - Topics Principaux : 1, 2, 4\n",
      "Question 106 - Topics Principaux : 4, 5, 6\n",
      "Question 107 - Topics Principaux : 5, 4, 3\n",
      "Question 108 - Topics Principaux : 4, 2, 6\n",
      "Question 109 - Topics Principaux : 1, 2, 4\n",
      "Question 110 - Topics Principaux : 5, 4, 3\n",
      "Question 111 - Topics Principaux : 5, 6, 1\n",
      "Question 112 - Topics Principaux : 2, 4, 1\n",
      "Question 113 - Topics Principaux : 5, 3, 4\n",
      "Question 114 - Topics Principaux : 0, 5, 3\n",
      "Question 115 - Topics Principaux : 4, 0, 6\n",
      "Question 116 - Topics Principaux : 1, 2, 0\n",
      "Question 117 - Topics Principaux : 2, 3, 6\n",
      "Question 118 - Topics Principaux : 0, 5, 6\n",
      "Question 119 - Topics Principaux : 2, 5, 4\n",
      "Question 120 - Topics Principaux : 4, 5, 6\n",
      "Question 121 - Topics Principaux : 3, 6, 2\n",
      "Question 122 - Topics Principaux : 4, 5, 3\n",
      "Question 123 - Topics Principaux : 4, 2, 5\n",
      "Question 124 - Topics Principaux : 0, 6, 4\n",
      "Question 125 - Topics Principaux : 2, 6, 0\n",
      "Question 126 - Topics Principaux : 5, 6, 3\n",
      "Question 127 - Topics Principaux : 0, 3, 1\n",
      "Question 128 - Topics Principaux : 0, 5, 3\n",
      "Question 129 - Topics Principaux : 3, 0, 1\n",
      "Question 130 - Topics Principaux : 0, 4, 1\n",
      "Question 131 - Topics Principaux : 1, 5, 6\n",
      "Question 132 - Topics Principaux : 2, 4, 1\n",
      "Question 133 - Topics Principaux : 0, 3, 1\n",
      "Question 134 - Topics Principaux : 6, 3, 5\n",
      "Question 135 - Topics Principaux : 0, 5, 4\n",
      "Question 136 - Topics Principaux : 4, 2, 1\n",
      "Question 137 - Topics Principaux : 5, 2, 3\n",
      "Question 138 - Topics Principaux : 4, 0, 6\n",
      "Question 139 - Topics Principaux : 1, 0, 4\n",
      "Question 140 - Topics Principaux : 5, 4, 0\n",
      "Question 141 - Topics Principaux : 5, 4, 0\n",
      "Question 142 - Topics Principaux : 0, 1, 4\n",
      "Question 143 - Topics Principaux : 0, 3, 6\n",
      "Question 144 - Topics Principaux : 0, 3, 4\n",
      "Question 145 - Topics Principaux : 6, 1, 0\n",
      "Question 146 - Topics Principaux : 4, 5, 1\n",
      "Question 147 - Topics Principaux : 4, 5, 2\n",
      "Question 148 - Topics Principaux : 0, 4, 1\n",
      "Question 149 - Topics Principaux : 5, 0, 6\n",
      "Question 150 - Topics Principaux : 2, 6, 5\n",
      "Question 151 - Topics Principaux : 0, 1, 6\n",
      "Question 152 - Topics Principaux : 1, 2, 4\n",
      "Question 153 - Topics Principaux : 5, 3, 6\n",
      "Question 154 - Topics Principaux : 4, 1, 3\n",
      "Question 155 - Topics Principaux : 4, 5, 3\n",
      "Question 156 - Topics Principaux : 4, 3, 1\n",
      "Question 157 - Topics Principaux : 1, 0, 6\n",
      "Question 158 - Topics Principaux : 6, 1, 0\n",
      "Question 159 - Topics Principaux : 5, 4, 0\n",
      "Question 160 - Topics Principaux : 2, 3, 0\n",
      "Question 161 - Topics Principaux : 3, 4, 0\n",
      "Question 162 - Topics Principaux : 1, 6, 3\n",
      "Question 163 - Topics Principaux : 3, 0, 4\n",
      "Question 164 - Topics Principaux : 2, 6, 3\n",
      "Question 165 - Topics Principaux : 4, 6, 2\n",
      "Question 166 - Topics Principaux : 5, 2, 3\n",
      "Question 167 - Topics Principaux : 3, 4, 2\n"
     ]
    }
   ],
   "source": [
    "num_questions = M_quest_topics_train.shape[0]\n",
    "num_top_topics = 3\n",
    "\n",
    "for question_idx in range(num_questions):\n",
    "    top_topics_idx = M_quest_topics_train[question_idx].argsort()[:-num_top_topics - 1:-1]\n",
    "    print(f\"Question {question_idx + 1} - Topics Principaux : {', '.join(map(str, top_topics_idx))}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:08:07.594501Z",
     "start_time": "2023-12-13T14:08:07.588692Z"
    }
   },
   "id": "fe3367a660c4970c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
