{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Approche Non Supervisée"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "224c5a5cade1ca7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_params):\n",
    "    with mlflow.start_run():\n",
    "        # Paramètres du modèle\n",
    "        for key, value in model_params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "\n",
    "def train_and_evaluate(X, y):\n",
    "    mlflow.start_run()\n",
    "    param_max_depth = 10\n",
    "    param_n_estimators = 100\n",
    "\n",
    "    # Enregistrez les paramètres\n",
    "    mlflow.log_param(\"max_depth\", param_max_depth)\n",
    "    mlflow.log_param(\"n_estimators\", param_n_estimators)\n",
    "\n",
    "    model = RandomForestClassifier(max_depth=param_max_depth, n_estimators=param_n_estimators)\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    mlflow.end_run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:58:04.971086Z",
     "start_time": "2023-12-13T16:58:03.056097Z"
    }
   },
   "id": "bdd1d8e3aa7fbc1f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train_data[['title', 'body']]\n",
    "y_train = train_data['tags']\n",
    "\n",
    "X_test = test_data[['title', 'body']]\n",
    "y_test = test_data['tags']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:58:04.986268Z",
     "start_time": "2023-12-13T16:58:04.971728Z"
    }
   },
   "id": "b13dba245c55668e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmatisation Title et Body"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c1245d4ada6694"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/0gx_zldn535fk5xp_qrwk8wr0000gn/T/ipykernel_29386/3531140223.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['title_lemmatized'] = X_train['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
      "/var/folders/cg/0gx_zldn535fk5xp_qrwk8wr0000gn/T/ipykernel_29386/3531140223.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['title_lemmatized'] = X_test['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "X_train['title_lemmatized'] = X_train['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "X_train['body_lemmatized'] = X_train['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "X_test['title_lemmatized'] = X_test['title'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n",
    "\n",
    "X_test['body_lemmatized'] = X_test['body'].apply(lambda text: ' '.join([token.lemma_ for token in nlp(text)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:58:11.362144Z",
     "start_time": "2023-12-13T16:58:04.987333Z"
    }
   },
   "id": "556b33d7b1cbd6c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21a7413cd00b067"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 245)\t2\n",
      "  (0, 316)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 170)\t1\n",
      "  (0, 203)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 256)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 291)\t1\n",
      "  (0, 347)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 241)\t1\n",
      "  (1, 118)\t1\n",
      "  (1, 145)\t1\n",
      "  (1, 188)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 156)\t2\n",
      "  (1, 215)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 57)\t1\n",
      "  (1, 207)\t1\n",
      "  :\t:\n",
      "  (163, 219)\t1\n",
      "  (163, 155)\t1\n",
      "  (164, 146)\t1\n",
      "  (164, 12)\t1\n",
      "  (164, 142)\t1\n",
      "  (164, 89)\t1\n",
      "  (164, 132)\t1\n",
      "  (164, 234)\t1\n",
      "  (164, 20)\t1\n",
      "  (164, 301)\t1\n",
      "  (164, 111)\t1\n",
      "  (164, 246)\t1\n",
      "  (164, 240)\t1\n",
      "  (165, 208)\t1\n",
      "  (165, 302)\t2\n",
      "  (165, 40)\t1\n",
      "  (165, 206)\t1\n",
      "  (165, 24)\t1\n",
      "  (165, 305)\t1\n",
      "  (165, 128)\t1\n",
      "  (166, 309)\t1\n",
      "  (166, 142)\t1\n",
      "  (166, 88)\t1\n",
      "  (166, 338)\t1\n",
      "  (166, 43)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_title = CountVectorizer()\n",
    "X_title_train = vectorizer_title.fit_transform(X_train['title_lemmatized'])\n",
    "X_title_test = vectorizer_title.transform(X_test['title_lemmatized'])\n",
    "\n",
    "vectorizer_body = CountVectorizer()\n",
    "X_body_train = vectorizer_body.fit_transform(X_train['body_lemmatized'])\n",
    "X_body_test = vectorizer_body.transform(X_test['body_lemmatized'])\n",
    "\n",
    "feature_names_title = vectorizer_title.get_feature_names_out()\n",
    "word_lists_title = [feature_names_title[idx].split() for idx in X_title_train.nonzero()[1]]\n",
    "\n",
    "print(X_title_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:58:11.380280Z",
     "start_time": "2023-12-13T16:58:11.363025Z"
    }
   },
   "id": "ae6da48cabf6adcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "# Concaténer title et body\n",
    "X_combined_train = vstack([X_title_train, X_body_train])\n",
    "X_combined_test = vstack([X_title_test, X_body_test])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1f63178ba088d26"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 245)\t2\n",
      "  (0, 316)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 170)\t1\n",
      "  (0, 203)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 256)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 291)\t1\n",
      "  (0, 347)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 241)\t1\n",
      "  (1, 118)\t1\n",
      "  (1, 145)\t1\n",
      "  (1, 188)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 156)\t2\n",
      "  (1, 215)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 57)\t1\n",
      "  (1, 207)\t1\n",
      "  :\t:\n",
      "  (163, 219)\t1\n",
      "  (163, 155)\t1\n",
      "  (164, 146)\t1\n",
      "  (164, 12)\t1\n",
      "  (164, 142)\t1\n",
      "  (164, 89)\t1\n",
      "  (164, 132)\t1\n",
      "  (164, 234)\t1\n",
      "  (164, 20)\t1\n",
      "  (164, 301)\t1\n",
      "  (164, 111)\t1\n",
      "  (164, 246)\t1\n",
      "  (164, 240)\t1\n",
      "  (165, 208)\t1\n",
      "  (165, 302)\t2\n",
      "  (165, 40)\t1\n",
      "  (165, 206)\t1\n",
      "  (165, 24)\t1\n",
      "  (165, 305)\t1\n",
      "  (165, 128)\t1\n",
      "  (166, 309)\t1\n",
      "  (166, 142)\t1\n",
      "  (166, 88)\t1\n",
      "  (166, 338)\t1\n",
      "  (166, 43)\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flo/anaconda3/lib/python3.11/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Removing duplicates in lists'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m RandomForestClassifier()\n\u001B[1;32m      5\u001B[0m model_params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_estimators\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m100\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m10\u001B[39m}\n\u001B[0;32m----> 6\u001B[0m train_and_evaluate_model(model, X_title_train, y_train, X_test, y_test, model_params)\n",
      "Cell \u001B[0;32mIn[1], line 17\u001B[0m, in \u001B[0;36mtrain_and_evaluate_model\u001B[0;34m(model, X_train, y_train, X_test, y_test, model_params)\u001B[0m\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Prédictions sur l'ensemble de test\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Enregistre les métriques\u001B[39;00m\n\u001B[1;32m     20\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:823\u001B[0m, in \u001B[0;36mForestClassifier.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m    803\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;124;03m    Predict class for X.\u001B[39;00m\n\u001B[1;32m    805\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    821\u001B[0m \u001B[38;5;124;03m        The predicted classes.\u001B[39;00m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 823\u001B[0m     proba \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict_proba(X)\n\u001B[1;32m    825\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    826\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_\u001B[38;5;241m.\u001B[39mtake(np\u001B[38;5;241m.\u001B[39margmax(proba, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:865\u001B[0m, in \u001B[0;36mForestClassifier.predict_proba\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    863\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    864\u001B[0m \u001B[38;5;66;03m# Check data\u001B[39;00m\n\u001B[0;32m--> 865\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_X_predict(X)\n\u001B[1;32m    867\u001B[0m \u001B[38;5;66;03m# Assign chunk of trees to jobs\u001B[39;00m\n\u001B[1;32m    868\u001B[0m n_jobs, _, _ \u001B[38;5;241m=\u001B[39m _partition_estimators(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_estimators, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:599\u001B[0m, in \u001B[0;36mBaseForest._validate_X_predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001B[39;00m\n\u001B[1;32m    598\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 599\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(X, dtype\u001B[38;5;241m=\u001B[39mDTYPE, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;129;01mand\u001B[39;00m (X\u001B[38;5;241m.\u001B[39mindices\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc \u001B[38;5;129;01mor\u001B[39;00m X\u001B[38;5;241m.\u001B[39mindptr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39mintc):\n\u001B[1;32m    601\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo support for np.int64 index based sparse matrices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    602\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 604\u001B[0m     out \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[1;32m    606\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:917\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    915\u001B[0m         array \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39mastype(array, dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    916\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 917\u001B[0m         array \u001B[38;5;241m=\u001B[39m _asarray_with_order(array, order\u001B[38;5;241m=\u001B[39morder, dtype\u001B[38;5;241m=\u001B[39mdtype, xp\u001B[38;5;241m=\u001B[39mxp)\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    920\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[1;32m    921\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcomplex_warning\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:380\u001B[0m, in \u001B[0;36m_asarray_with_order\u001B[0;34m(array, dtype, order, copy, xp)\u001B[0m\n\u001B[1;32m    378\u001B[0m     array \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39marray(array, order\u001B[38;5;241m=\u001B[39morder, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 380\u001B[0m     array \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39masarray(array, order\u001B[38;5;241m=\u001B[39morder, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m xp\u001B[38;5;241m.\u001B[39masarray(array)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2084\u001B[0m, in \u001B[0;36mNDFrame.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m   2082\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype: npt\u001B[38;5;241m.\u001B[39mDTypeLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m   2083\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 2084\u001B[0m     arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(values, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   2085\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2086\u001B[0m         astype_is_view(values\u001B[38;5;241m.\u001B[39mdtype, arr\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   2087\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write()\n\u001B[1;32m   2088\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mis_single_block\n\u001B[1;32m   2089\u001B[0m     ):\n\u001B[1;32m   2090\u001B[0m         \u001B[38;5;66;03m# Check if both conversions can be done without a copy\u001B[39;00m\n\u001B[1;32m   2091\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m astype_is_view(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtypes\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m], values\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mand\u001B[39;00m astype_is_view(\n\u001B[1;32m   2092\u001B[0m             values\u001B[38;5;241m.\u001B[39mdtype, arr\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m   2093\u001B[0m         ):\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'Removing duplicates in lists'"
     ]
    }
   ],
   "source": [
    "print(X_title_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model_params = {'n_estimators': 100, 'max_depth': 10}\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "train_and_evaluate_model(model, X_title_train, y_train, X_test, y_test, model_params)\n",
    "\n",
    "#train_and_evaluate(X_title_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:58:12.512350Z",
     "start_time": "2023-12-13T16:58:11.381034Z"
    }
   },
   "id": "c4e7900b6eac186a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-13T16:58:12.511534Z"
    }
   },
   "id": "827e28d61a586453"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
